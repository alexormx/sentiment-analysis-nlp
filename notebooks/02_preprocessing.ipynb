{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff95283",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Preprocesamiento del Dataset IMDB\n",
    "\n",
    "En este notebook realizamos el **preprocesamiento de texto** para el modelo de clasificaciÃ³n de sentimiento de reseÃ±as de pelÃ­culas IMDb. Esta etapa incluye:\n",
    "\n",
    "1. Cargar datos limpios desde CSV\n",
    "2. Procesar etiquetas\n",
    "3. Tokenizar texto (vectorizaciÃ³n)\n",
    "4. Aplicar padding\n",
    "5. Guardar arrays procesados para entrenamiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a26a8",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Preprocesamiento del Dataset IMDB\n",
    "\n",
    "En este paso realizamos la tokenizaciÃ³n, padding y codificaciÃ³n de etiquetas para preparar los datos para entrenamiento. TambiÃ©n guardamos los resultados para su reutilizaciÃ³n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c52436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 16:39:27.261749: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-17 16:39:28.038709: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-17 16:39:30.205117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# 1. Importar librerÃ­as\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1c247",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Cargar datos limpios\n",
    "\n",
    "Utilizamos el archivo `clean_imdb_100k.csv`, que contiene reseÃ±as ya limpiadas (sin HTML, puntuaciÃ³n ni stopwords) y con su respectiva etiqueta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b64fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          clean_text  label\n",
      "0  run do not look back i made the mistake of buy...      1\n",
      "1  excellent book with pictures for teens to lear...      2\n",
      "2  improving your quality of relationships maxwel...      2\n",
      "3  no price no need to buy if amazon is unwilling...      1\n",
      "4  great toy holder this is an awesome product fo...      2\n"
     ]
    }
   ],
   "source": [
    "# 2. Cargar el dataset limpio desde CSV\n",
    "df = pd.read_csv(\"../data/clean_imdb_100k.csv\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ee918",
   "metadata": {},
   "source": [
    "3. ðŸ”¹ Preparar etiquetas (opcional: convertir a 0 y 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b02f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"label\"] - 1  # AsegÃºrate que la etiqueta original era 1 y 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f47da",
   "metadata": {},
   "source": [
    "4. ðŸ”¹ TokenizaciÃ³n y conversiÃ³n a secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b4cf98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"clean_text\"])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea08976",
   "metadata": {},
   "source": [
    "5. ðŸ”¹ Padding de secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5858dabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Padding listo. Shape: (100000, 200)\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, maxlen=200, padding=\"post\", truncating=\"post\")\n",
    "print(\"âœ… Padding listo. Shape:\", padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f03ff1",
   "metadata": {},
   "source": [
    "6. ðŸ”¹ Guardar datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81ab6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Datos guardados en ../data/processed_data.npz\n"
     ]
    }
   ],
   "source": [
    "np.savez_compressed(\"../data/processed_data.npz\", X=padded, y=labels)\n",
    "print(\"âœ… Datos guardados en ../data/processed_data.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07aad9b",
   "metadata": {},
   "source": [
    "7. ðŸ”¹ Guardar el tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1573cf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenizer guardado en ../data/tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"âœ… Tokenizer guardado en ../data/tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "601bbab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Datos divididos y guardados correctamente.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir en entrenamiento y validaciÃ³n\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Guardar conjuntos de datos\n",
    "np.savez_compressed(\"../data/processed_split.npz\", \n",
    "                    X_train=X_train, X_val=X_val, \n",
    "                    y_train=y_train, y_val=y_val)\n",
    "\n",
    "print(\"âœ… Datos divididos y guardados correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266fb9d5",
   "metadata": {},
   "source": [
    "## ðŸ“Š EstadÃ­sticas del corpus\n",
    "\n",
    "AquÃ­ mostramos estadÃ­sticas bÃ¡sicas del texto procesado y el tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc34f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NÃºmero de secuencias: 100000\n",
      "âœ… Longitud promedio de secuencia: 77.87445\n",
      "âœ… Longitud mÃ¡xima: 216\n",
      "âœ… TamaÃ±o del vocabulario: 190637\n"
     ]
    }
   ],
   "source": [
    "# EstadÃ­sticas Ãºtiles\n",
    "sequence_lengths = [len(seq) for seq in sequences]\n",
    "print(\"âœ… NÃºmero de secuencias:\", len(sequences))\n",
    "print(\"âœ… Longitud promedio de secuencia:\", np.mean(sequence_lengths))\n",
    "print(\"âœ… Longitud mÃ¡xima:\", np.max(sequence_lengths))\n",
    "print(\"âœ… TamaÃ±o del vocabulario:\", len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61591a4b",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Guardar arrays procesados\n",
    "\n",
    "Guardamos los datos como archivo `.npz` para usarlos en el notebook de entrenamiento (`03_train_model.ipynb`) y tambiÃ©n exportamos el CSV limpio.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
